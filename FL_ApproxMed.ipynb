{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-gabriel",
   "metadata": {
    "executionInfo": {
     "elapsed": 4081,
     "status": "ok",
     "timestamp": 1633613839705,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "studied-gabriel"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-lounge",
   "metadata": {},
   "source": [
    "#### Modified from below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-devil",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Ashwin R Jadhav\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-norman",
   "metadata": {
    "id": "different-norman"
   },
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-talent",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1633613839706,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "specialized-talent"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_input = nn.Linear(28*28, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.layer_hidden = nn.Linear(512, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.shape[1]*x.shape[-2]*x.shape[-1])\n",
    "        x = self.layer_input(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_hidden(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-longitude",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1633613839707,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "attractive-longitude"
   },
   "outputs": [],
   "source": [
    "class CNNMnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMnist, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-genesis",
   "metadata": {
    "id": "original-genesis"
   },
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-given",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1633613839708,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "norman-given"
   },
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-basketball",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1633613839708,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "owned-basketball"
   },
   "outputs": [],
   "source": [
    "def mnist_iid(dataset, num_users):\n",
    "    \"\"\"\n",
    "    Sample I.I.D. client data from MNIST dataset\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: dict of image index\n",
    "    \"\"\"\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs, num_items,\n",
    "                                             replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-clinton",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1633613839709,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "sorted-clinton"
   },
   "outputs": [],
   "source": [
    "def get_dataset(dataset):\n",
    "    \"\"\" Returns train and test datasets and a user group which is a dict where\n",
    "    the keys are the user index and the values are the corresponding data for\n",
    "    each of those users.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == 'mnist' or 'fmnist':\n",
    "        if dataset == 'mnist':\n",
    "            data_dir = '../data/mnist/'\n",
    "        else:\n",
    "            data_dir = '../data/fmnist/'\n",
    "\n",
    "        apply_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        \n",
    "        if dataset == 'mnist':\n",
    "            train_dataset = datasets.MNIST(data_dir, train=True, download=True,\n",
    "                                           transform=apply_transform)\n",
    "\n",
    "            test_dataset = datasets.MNIST(data_dir, train=False, download=True,\n",
    "                                          transform=apply_transform)\n",
    "        else:\n",
    "            train_dataset = datasets.FashionMNIST(data_dir, train=True, download=True,\n",
    "                                           transform=apply_transform)\n",
    "\n",
    "            test_dataset = datasets.FashionMNIST(data_dir, train=False, download=True,\n",
    "                                          transform=apply_transform)            \n",
    "\n",
    "        user_groups = mnist_iid(train_dataset, 100)\n",
    "\n",
    "    return train_dataset, test_dataset, user_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-williams",
   "metadata": {
    "id": "elect-williams"
   },
   "source": [
    "#### Local Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-coach",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1633613839709,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "peripheral-coach"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-public",
   "metadata": {
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1633613840060,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "returning-public"
   },
   "outputs": [],
   "source": [
    "class LocalUpdate(object):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.trainloader, self.validloader, self.testloader = self.train_val_test(\n",
    "            dataset, list(idxs))\n",
    "        self.device = device\n",
    "        # Default criterion set to NLL loss function\n",
    "        self.criterion = nn.NLLLoss().to(self.device)\n",
    "\n",
    "    def train_val_test(self, dataset, idxs):\n",
    "        \"\"\"\n",
    "        Returns train, validation and test dataloaders for a given dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        # split indexes for train, validation, and test (80, 10, 10)\n",
    "        idxs_train = idxs[:int(0.8*len(idxs))]\n",
    "        idxs_val = idxs[int(0.8*len(idxs)):int(0.9*len(idxs))]\n",
    "        idxs_test = idxs[int(0.9*len(idxs)):]\n",
    "\n",
    "        trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
    "                                 batch_size=10, shuffle=True)\n",
    "        validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
    "                                 batch_size=int(len(idxs_val)/10), shuffle=False)\n",
    "        testloader = DataLoader(DatasetSplit(dataset, idxs_test),\n",
    "                                batch_size=int(len(idxs_test)/10), shuffle=False)\n",
    "        return trainloader, validloader, testloader\n",
    "\n",
    "    def update_weights(self, local_ep, model, attack):\n",
    "        # Set mode to train model\n",
    "        model.train()\n",
    "\n",
    "        # Set optimizer for the local updates\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01,\n",
    "                                    momentum=0.5)\n",
    "\n",
    "        for iter in range(local_ep):\n",
    "\n",
    "            for batch_idx, (images, labels) in enumerate(self.trainloader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                if attack == 'label':\n",
    "                    labels = torch.zeros(len(labels), dtype = torch.long).to(self.device)\n",
    "                model.zero_grad()\n",
    "                log_probs = model(images)\n",
    "                loss = self.criterion(log_probs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return model.state_dict()\n",
    "\n",
    "def test_inference(model, test_dataset):\n",
    "    \"\"\" Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "\n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "    testloader = DataLoader(test_dataset, batch_size=128,\n",
    "                            shuffle=False)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(testloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(images)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "\n",
    "    accuracy = correct/total\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-decision",
   "metadata": {
    "id": "formed-decision"
   },
   "source": [
    "#### Aggregators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-expression",
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1633613850176,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "duplicate-expression"
   },
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-humidity",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1633613850539,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "paperback-humidity"
   },
   "outputs": [],
   "source": [
    "def median(w):\n",
    "    w_med = copy.deepcopy(w[0])\n",
    "    n = len(w)\n",
    "    for k in w_med.keys():\n",
    "        if 'bias' in k:\n",
    "            blank = torch.zeros(n, len(w_med[k]))\n",
    "            for i in range(n):\n",
    "                blank[i] = w[i][k]\n",
    "            w_med[k] = torch.tensor(np.median(blank, axis = -2))\n",
    "        elif 'weight' in k:\n",
    "            if 'conv' in k:\n",
    "                k1, k2, k3, k4 = w_med[k].size()\n",
    "                blank = torch.zeros(n, k1, k2, k3, k4)\n",
    "                for i in range(n):\n",
    "                    blank[i] = w[i][k]\n",
    "                w_med[k] = torch.tensor(np.median(blank, axis = -5))\n",
    "            else:\n",
    "                k1, k2 = w_med[k].size()\n",
    "                blank = torch.zeros(n, k1, k2)\n",
    "                for i in range(n):\n",
    "                    blank[i] = w[i][k]\n",
    "                w_med[k] = torch.tensor(np.median(blank, axis = -3))\n",
    "    return w_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-pregnancy",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1633613850937,
     "user": {
      "displayName": "남치현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04014407494601346902"
     },
     "user_tz": -540
    },
    "id": "varying-pregnancy"
   },
   "outputs": [],
   "source": [
    "def approxmed5(w):\n",
    "    w_med = copy.deepcopy(w[0])\n",
    "    n = len(w)\n",
    "    for k in w_med.keys():\n",
    "        if 'bias' in k:\n",
    "            k1 = len(w_med[k])\n",
    "            \n",
    "            blank = torch.zeros(n, k1)\n",
    "            index = np.random.permutation(n)\n",
    "            \n",
    "            for i in range(n):\n",
    "                blank[i] = w[index[i]][k]\n",
    "                \n",
    "            result1 = torch.zeros(n//5, k1)\n",
    "            for i in range(n//5):\n",
    "                result1[i] = torch.tensor(np.median(blank[5*i:5*i+5], axis = -2))\n",
    "                \n",
    "            result2 = torch.zeros(4, k1)\n",
    "            \n",
    "            for i in range(4):\n",
    "                result2[i] = torch.tensor(np.median(result1[5*i:5*i+5], axis = -2))\n",
    "                \n",
    "            w_med[k] = torch.tensor(np.median(result2, axis = -2))\n",
    "            \n",
    "        elif 'weight' in k:\n",
    "            if 'conv' in k:\n",
    "                k1, k2, k3, k4 = w_med[k].size()\n",
    "\n",
    "                blank = torch.zeros(n, k1, k2, k3, k4)\n",
    "                index = np.random.permutation(n)\n",
    "\n",
    "                for i in range(n):\n",
    "                    blank[i] = w[index[i]][k]\n",
    "\n",
    "                result1 = torch.zeros(n//5, k1, k2, k3, k4)\n",
    "                for i in range(n//5):\n",
    "                    result1[i] = torch.tensor(np.median(blank[5*i:5*i+5], axis = -5))\n",
    "\n",
    "                result2 = torch.zeros(4, k1, k2, k3, k4)\n",
    "\n",
    "                for i in range(4):\n",
    "                    result2[i] = torch.tensor(np.median(result1[5*i:5*i+5], axis = -5))\n",
    "\n",
    "                w_med[k] = torch.tensor(np.median(result2, axis = -5))\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                k1, k2 = w_med[k].size()\n",
    "\n",
    "                blank = torch.zeros(n, k1, k2)\n",
    "                index = np.random.permutation(n)\n",
    "\n",
    "                for i in range(n):\n",
    "                    blank[i] = w[index[i]][k]\n",
    "\n",
    "                result1 = torch.zeros(n//5, k1, k2)\n",
    "                for i in range(n//5):\n",
    "                    result1[i] = torch.tensor(np.median(blank[5*i:5*i+5], axis = -3))\n",
    "\n",
    "                result2 = torch.zeros(4, k1, k2)\n",
    "\n",
    "                for i in range(4):\n",
    "                    result2[i] = torch.tensor(np.median(result1[5*i:5*i+5], axis = -3))\n",
    "\n",
    "                w_med[k] = torch.tensor(np.median(result2, axis = -3))\n",
    "            \n",
    "    return w_med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-indie",
   "metadata": {},
   "source": [
    "#### Global update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-housing",
   "metadata": {
    "id": "photographic-housing"
   },
   "outputs": [],
   "source": [
    "def global_update(epochs, local_ep, dataset, model, aggregator, attack):\n",
    "    error_list = []\n",
    "    # load dataset and user groups\n",
    "    train_dataset, test_dataset, user_groups = get_dataset(dataset)\n",
    "\n",
    "    # BUILD MODEL\n",
    "    if model == 'cnn':\n",
    "        # Convolutional neural netork\n",
    "        if dataset == 'mnist':\n",
    "            global_model = CNNMnist()\n",
    "        elif dataset == 'fmnist':\n",
    "            global_model = CNNMnist()\n",
    "\n",
    "    elif model == 'mlp':\n",
    "        # Multi-layer preceptron\n",
    "        img_size = train_dataset[0][0].shape\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "            global_model = MLP()\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "\n",
    "    # Set the model to train and send it to device.\n",
    "    global_model.to(device)\n",
    "    global_model.train()\n",
    "    print(global_model)\n",
    "\n",
    "    # copy weights\n",
    "    global_weights = global_model.state_dict()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        local_weights = []\n",
    "        print(f'\\n | Global Training Round : {epoch+1} |\\n')\n",
    "\n",
    "        global_model.train()\n",
    "\n",
    "        idxs_users = np.random.choice(range(100), 100, replace=False)\n",
    "        \n",
    "        malicious = 0\n",
    "        i = 0\n",
    "        for idx in idxs_users:\n",
    "            print(i)\n",
    "            i += 1\n",
    "\n",
    "            local_model = LocalUpdate(dataset=train_dataset, idxs=user_groups[idx])\n",
    "            w = local_model.update_weights(local_ep, copy.deepcopy(global_model), 'clean')\n",
    "            \n",
    "            \n",
    "            if attack == 'negative':\n",
    "                if malicious < 30:\n",
    "                    send = copy.deepcopy(w)\n",
    "                    for k in send.keys():\n",
    "                        send[k] = -2*w[k]\n",
    "                    local_weights.append(copy.deepcopy(send))\n",
    "                else:\n",
    "                    local_weights.append(copy.deepcopy(w))\n",
    "            else:\n",
    "                local_weights.append(copy.deepcopy(w))\n",
    "            malicious += 1\n",
    "\n",
    "        # update global weights\n",
    "        if aggregator == 'average':\n",
    "            global_weights = average_weights(local_weights)\n",
    "        elif aggregator == 'median':\n",
    "            global_weights = median(local_weights)\n",
    "        elif aggregator == 'approxmed5':\n",
    "            global_weights = approxmed5(local_weights)\n",
    "\n",
    "        # update global weights\n",
    "        global_model.load_state_dict(global_weights)\n",
    "        global_model.eval()\n",
    "    # Test inference after completion of training\n",
    "        test_acc, test_loss = test_inference(copy.deepcopy(global_model), test_dataset)\n",
    "\n",
    "        print(f' \\n Results after {epoch} global rounds of training:')\n",
    "        print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "        error_list.append(1-test_acc)\n",
    "    return error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-allah",
   "metadata": {},
   "source": [
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = global_update(10,10,'fmnist','mlp','approxmed5','clean')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FL_ApproxMed.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
